---
layout: post
title: 用 sklearn 进行文本分析
tags:
  - cs
date: 2025-5-8 ‏17:22:37 +0800
published: false
---





## 统计词频


### 词频分布可视化

可以统计文本中单词的频率，并使用柱状图展示前 N 个最常见的单词。

  

python

运行

```python
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# 示例文本
corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third one.',
    'Is this the first document?'
]

# 使用 CountVectorizer 提取词频
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
feature_names = vectorizer.get_feature_names_out()
word_counts = X.sum(axis=0).A1

# 创建 DataFrame
df = pd.DataFrame({'word': feature_names, 'count': word_counts})
# 按词频降序排序并取前 10 个
top_words = df.sort_values(by='count', ascending=False).head(10)

# 绘制柱状图
plt.figure(figsize=(10, 6))
plt.bar(top_words['word'], top_words['count'])
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Top 10 Most Frequent Words')
plt.xticks(rotation=45)
plt.show()
```



文本分类结果可视化可以使用混淆矩阵来展示分类模型的性能，混淆矩阵可以直观地显示模型在各个类别上的分类情况。



文本聚类结果可视化
对于文本聚类结果，可以使用散点图展示降维后的文本向量在二维平面上的分布，不同的聚类用不同的颜色表示。

### 提取关键词

可以用 tf-idf 来修正词频。

## 文本分类

### 用关键词

可以用正则表达式。

import regex as re


### 用 TF-IDF

tf-idf 可用于文本分类。
```python
# 使用TF-IDF进行文本分类
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score
from sklearn.pipeline import Pipeline

# 1. 加载数据集（使用20个新闻组数据集）
# 选择几个类别以加快训练速度
categories = ['rec.sport.hockey', 'talk.politics.misc', 
              'comp.graphics', 'sci.med']
newsgroups = fetch_20newsgroups(subset='all', categories=categories,
                                remove=('headers', 'footers', 'quotes'))

X = newsgroups.data  # 文本数据
y = newsgroups.target  # 标签

# 2. 分割训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 3. 创建一个包含TF-IDF和分类器的管道
# 这样可以确保在交叉验证和测试时使用相同的参数
text_clf = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english', max_features=5000)),
    ('clf', MultinomialNB()),
])

# 4. 训练模型
text_clf.fit(X_train, y_train)

# 5. 在测试集上进行预测
y_pred = text_clf.predict(X_test)

# 6. 评估模型性能
print(f"准确率: {accuracy_score(y_test, y_pred):.4f}")
print("\n分类报告:")
print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))

# 7. 示例预测
sample_texts = [
    "The goalkeeper made a great save in the last minute of the game",
    "The new computer graphics card has 8GB of VRAM",
    "The president announced new healthcare policies yesterday",
    "The patient was diagnosed with a rare form of cancer"
]

predicted_categories = text_clf.predict(sample_texts)
for text, category in zip(sample_texts, predicted_categories):
    print(f"\n文本: {text}")
    print(f"预测类别: {newsgroups.target_names[category]}")

```

 


## 文本聚类

### KMeans

散点图。用于分类模型。

```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt

# 加载数据集
categories = ['alt.atheism', 'soc.religion.christian']
data = fetch_20newsgroups(subset='all', categories=categories)

# 特征提取
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(data.data)

# 聚类
kmeans = KMeans(n_clusters=2, random_state=0)
kmeans.fit(X)

# 降维
svd = TruncatedSVD(n_components=2)
X_reduced = svd.fit_transform(X)

# 可视化聚类结果
plt.figure(figsize=(10, 6))
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=kmeans.labels_, cmap='viridis')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.title('Text Clustering Results')
plt.show()

```
### LDA
- 
- 可以使用柱状图展示每个主题下的前 N 个关键词及其权重。

```python
# 属于一种文本聚类
import numpy as np
from sklearn.datasets import fetch_20newsgroups  
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation


# 使用 sklearn 提供的新闻数据集
newsgroups_data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))
texts = newsgroups_data.data[:1000]  # 获取新闻数据集的文本内容
print(len(texts))

# 文本向量化
vectorizer = CountVectorizer(stop_words='english')  # 添加 stop_words='english' 参数以去除停用词
X = vectorizer.fit_transform(texts)

# 训练LDA模型
lda = LatentDirichletAllocation(n_components=5)  # 主题数量
         
lda.fit(X)

# 查看每个主题的前几个关键词
n_top_words = 5
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(lda.components_):
    print(f"主题 {topic_idx}:")
    top_words_idx = topic.argsort()[:-n_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_words_idx]
    print(", ".join(top_words))
    print([topic[i] for i in top_words_idx])

# 预测每个文档的主题分布
doc_topic_distribution = lda.transform(X)
print("每个文档的主题分布：")
print(doc_topic_distribution)

```
## 可视化



参考资料
NLTK
Spacy
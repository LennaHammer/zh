---
date: 2025-5-8 ‏20:00:00 +0800
---


### pytorch 与 deep learning

pytorch 运算
- 改变形状 
	- view
	- `reshape(-1)`
	- `inputs.unsqueeze(-1)`  `unsqueeze()` 和 `squeeze()` 
	- `transpose()` 方法可用于交换张量的两个维度
		- permute()  
	- expand() 和 repeat()
- 矩阵运算，注意存在 batch 维。
	- a + b 向量相加，维度相等（或者一个是标量）。
	-  input_gru = torch.cat((embedded, context), dim=2) 两个特征合并。
- 层，包括 batch 维，很多 torch 的运算都隐含 batch 维，除了一些基础的矩阵运算。
	- Sequential 
	- Linear(28 * 28, 128), 
	- ReLU() 放在 Linear 之后
	- CNN 用于图像，也用于并行的场景。
- 完整的模型也可以作为一个层来调用。
	- 注意输入输出格式，和约定。数组格式。可能需要转换。可能有多个输入，可能输入是个元组或者字典。
		- 调用第三模型的时候根据文档和示例中描述
	- 图像
		- `models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights="DEFAULT")`
		- cnn 用于提取特征。
	- 文本
		- `BertForSequenceClassification.from_pretrained("bert-base-uncased")`
		- `LSTM(input_dim, hidden_dim, batch_first=True)` 输入是 序列x词向量。需要对循环输入。
		- embedding 用于词表转词向量。
- 损失函数，也是一种层
	- mse_loss 用于拟合任务
		- `loss = self.model(batch).sum()`
	- CrossEntropyLoss() 用于多分类任务，注意模型最后不需要激活函数，不需要 softmax，已经隐含在损失函数内了。
	- 二分类也可以当作多分类处理。
- 优化器
	- `optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)`
	- `optim.SGD(self.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)`
- 评估
	- `preds = self.model(imgs).argmax(dim=-1)` 从向量得到编号。
	- `acc += (preds == labels).sum().item()` 得到正确的个数
	- 机器翻译的评价指标 BLUE。
	- 评价指标和损失函数不一定是同一个。因为实现差异损失函数的具体大小没有实际意义。
	- 评估首先是肉眼看一下结果，甚至不一定是数字指标。
- 归一化
	-  nn.BatchNorm2d(c_out), 位置在线性层后。
	-  层归一化。 nn.LayerNorm(self.hparams.model_dim),  位置在线性层后。
- 训练技巧
	- 学习率调整
	- dropout nn.Dropout(dropout) 位置在线性层前。
	- 训练过程中打印损失函数变化，画出折线图。
	- torch.nn.utils.clip_grad_norm_(...) 防止梯度爆炸
- 数据集
	- DataSet 相当于一个数组。
	- DataLoader 用来分 batch。指定 batch size。序列可能是填 0。
	- 可能需要预处理或者增强。图像 transform（翻转、缩放），文本 tokenize（词典转编号）。在 dataloader 之前可以提升性能。
	- 划分训练、测试/验证。
	- 文本的输入是字符编号，然后用 embedding 变成向量。
	- 见后面的代码，利用 pytorch 或其他库提供的工具。
- 备注
	- 通常实现为命令行模式方便调用。
	- 训练过程大多数情况不需要手工调整。
	- 首先找一个完整的代码跑一下。
	- 动态图很方便交互调试，用固定的输入来尝试调用，看对应输出的格式。
	- 对于整个过程只关心必要的部分。
	- 对模块拼接，不要从零开始。
	- 调用第三方模块的时候，看一下输入输出格式，以及约定。这个不固定，特别是有细节差异。有预训练权重。
- 常见任务
	- 文本。分类。总结。
		- 文本分类 数据集 AG NEWS 新闻分类，IMDB 评论 情感分类 模型 rnn/lstm，fasttext, bert, 
		- 序列生成，翻译。rnn/lstm, bert, gpt 
	- 图像。分类。检测。
		- 图像分类 MNIST 手写数字识别 模型 cnn, resnet, 
		- 目标检测 模型 yolo
	- 传统模型
		- 文本分类 tfidf+svm
		- 
- 发展历史
	- 图像分类 cnn, vgg, resnet,
	- 文本
		- 词向量
		- 序列模型 rnn, lstm, attention, bert, gpt, 
- 代码参考
	- https://lightning.ai/docs/pytorch/stable/
	- https://lightning.ai/docs/pytorch/stable/tutorials.html
	- [Annotated Research Paper Implementations](https://nn.labml.ai/)
	- [PyTorch Tutorials](https://pytorch.org/tutorials/)
	- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- 其他补充
	- img = img.unsqueeze(0) 用来添加 batch
	- topk 用来得到最大的下边编号。
	- 特征层可以固定权重以减少微调开销。
	- 图像输入注意格式，尺寸，通道，归一化。
	- 用 imshow 的时候要根据预处理逆转换回去。
	- model_ft = models.resnet18(weights='IMAGENET1K_V1')
	- bert 可以句子分类，可以词分类，可以序列生成。



### 学习话题

- 论文
	- [Attention Is All You Need](https://ar5iv.labs.arxiv.org/html/1706.03762)
	- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://ar5iv.labs.arxiv.org/html/1810.04805)
- chatgpt
	- 调用工具
	- 通过 api 使用，输入和返回是文本。分类任务可以返回标签字符串。 
	- 利用 示例和微调 都可以提升具体任务的效果。根据数据量选择不同的方案。
- 其他机器学习库
	- sklearn 传统的机器学习
	- waka
	- pytorch
	- keras 基于 tf 已淘汰，
	- hugging face
	- 图模型
	- 词向量，主题模型 LDA。
	- numpy  提供基本的矩阵运算
	- scipy 有最优化工具箱。
- 文本上的算法。
	- 文本分类
- 教材
	- 李航。
	- 文本上的算法。
	- hulu 葫芦书（两本）有点老，但是有一些有趣的细节问题。答案仅参考，不一定好。
- 论文
	- 问题是什么，和基本的问题有什么区别，评估方法是什么。
	- 传统方法是什么，相似问题上的方法是是什么，在哪个基础方法上做出哪些改进。
	- 如何评估，有什么实际应用，分析各个组件的作用和场景。
	- 一些 survey。

### 代码


- Hello world 	Pretrain - Hello world example 	Open In Studio
- Image classification 	Finetune - ResNet-34 model to classify images of cars 	Open In Studio
- Image segmentation 	Finetune - ResNet-50 model to segment images 	Open In Studio
- Object detection 	Finetune - Faster R-CNN model to detect objects 	Open In Studio
- Text classification 	Finetune - text classifier (BERT model) 	Open In Studio
- Text summarization 	Finetune - text summarization (Hugging Face transformer model) 	Open In Studio
- Audio generation 	Finetune - audio generator (transformer model) 	Open In Studio
- LLM finetuning 	Finetune - LLM (Meta Llama 3.1 8B) 	Open In Studio
- Image generation 	Pretrain - Image generator (diffusion model) 	Open In Studio
- Recommendation system 	Train - recommendation system (factorization and embedding) 	Open In Studio
- Time-series forecasting 	Train - Time-series forecasting with LSTM






#### 层/模块

pytorch-lighting 也一样
```python
class LitAutoEncoder(L.LightningModule):
    def __init__(self):
        super().__init__()
		self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))
		self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))

	def forward(self, batch, batch_idx):
		x, _ = batch
		x = x.view(x.size(0), -1)
		z = self.encoder(x)
		x_hat = self.decoder(z)
		loss = F.mse_loss(x_hat, x)
		self.log("train_loss", loss)
		return loss
```
#### 数据集

图像数据集
```python
DEFAULT_TRANSFORM = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((224, 224)),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
train_dataset = datasets.StanfordCars(root=".", download=False, transform=DEFAULT_TRANSFORM)
torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=5)
```
文本数据集
```python
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
dataset = load_dataset("imdb")["train"]
dataset = dataset.map(
	lambda sample: tokenizer(sample["text"], padding="max_length", truncation=True))
dataset.set_format(type="torch")
return torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)
```
cnn

fcn


vgg

BatchNorm 在 线性层后激活层前
```python
self.net = nn.Sequential(
	nn.Conv2d(
		c_in, c_out, kernel_size=3, padding=1, stride=1 if not subsample else 2, bias=False
	),  # No bias needed as the Batch Norm handles it
	nn.BatchNorm2d(c_out),
	act_fn(),
	nn.Conv2d(c_out, c_out, kernel_size=3, padding=1, bias=False),
	nn.BatchNorm2d(c_out),
)
```
resnet


核心之一是残差网络

```python
def forward(self, x):
	z = self.net(x)
	if self.downsample is not None:
		x = self.downsample(x)
	out = z + x
	out = self.act_fn(out)
	return out
```

DQN


```python
# DQN


```

评价指标

```
loss = F.cross_entropy(preds.view(-1, preds.size(-1)), labels.view(-1))
acc = (preds.argmax(dim=-1) == labels).float().mean()
```


代码


#### CNN 图像分类
- CNN 常用卷积核 3， pool 后尺寸减半。
- 最后 flatten 后进入线性层
- 训练时对输入图像增强。
- pool 层缩小大小。
	- nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Linear(c_hidden[-1], self.hparams.num_classes)

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader


# 定义 CNN 模型
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(2)
        self.fc1 = nn.Linear(32 * 7 * 7, 128)
        self.relu3 = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool1(self.relu1(self.conv1(x)))
        x = self.pool2(self.relu2(self.conv2(x)))
        x = x.view(-1, 32 * 7 * 7)
        x = self.relu3(self.fc1(x))
        x = self.fc2(x)
        return x


# 数据预处理
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# 加载训练集和测试集
train_dataset = datasets.MNIST(root='./data', train=True,
                               download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False,
                              download=True, transform=transform)

# 创建数据加载器
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# 初始化模型、损失函数和优化器
model = SimpleCNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for i, (images, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')

# 测试模型
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1) # 也可以用 argmax
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy on test set: {100 * correct / total}%')
    
```



FastText 文本分类
- 对文本向量取平均。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np


# 定义数据集类
class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)


# 定义 FastText 模型
class FastText(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_classes):
        super(FastText, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.fc = nn.Linear(embedding_dim, num_classes)

    def forward(self, x):
        embedded = self.embedding(x)
        pooled = torch.mean(embedded, dim=1)
        output = self.fc(pooled)
        return output


# 模拟数据
vocab_size = 1000
embedding_dim = 100
num_classes = 2
num_samples = 1000
max_length = 20

texts = [np.random.randint(0, vocab_size, max_length) for _ in range(num_samples)]
labels = np.random.randint(0, num_classes, num_samples)

# 创建数据集和数据加载器
dataset = TextDataset(texts, labels)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# 初始化模型、损失函数和优化器
model = FastText(vocab_size, embedding_dim, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 10
for epoch in range(num_epochs):
    running_loss = 0.0
    for texts, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader)}')

```



#### Attention
- Q,K,V 是 $batch \times length \times dim$ 
- $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
- self attention 即 Q=K=V 用于 编码
- decoder 可以用来给输入添加上下文。

```python
def scaled_dot_product(q, k, v, mask=None):
    d_k = q.size()[-1]
    attn_logits = torch.matmul(q, k.transpose(-2, -1))
    attn_logits = attn_logits / math.sqrt(d_k)
    if mask is not None:
        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)
    attention = F.softmax(attn_logits, dim=-1)
    values = torch.matmul(attention, v)
    return values, attention

seq_len, d_k = 3, 2
pl.seed_everything(42)
q = torch.randn(seq_len, d_k)
k = torch.randn(seq_len, d_k)
v = torch.randn(seq_len, d_k)
values, attention = scaled_dot_product(q, k, v)
print("Q\n", q)
print("K\n", k)
print("V\n", v)
print("Values\n", values)
print("Attention\n", attention)
```



```python
import torch
import torch.nn.functional as F

def dot_product_attention(Q, K, V, mask=None):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    return output, attention_weights
```



#### seq2seq
- rnn 需要对字符逐个编码，也需要逐个解码（直到结束标识）。实现为循环。
- rnn可以替换为lstm。
- 解码的损失函数是下一词的话可以并行。

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random

# 简单的词汇表
input_vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, 'hello': 3, 'world': 4}
output_vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, 'bonjour': 3, 'monde': 4}

input_vocab_size = len(input_vocab)
output_vocab_size = len(output_vocab)

# 示例数据
input_seqs = [['hello', 'world'], ['hello']]
target_seqs = [['bonjour', 'monde'], ['bonjour']]

# 将文本转换为索引序列
def text_to_indices(text, vocab):
    indices = [vocab['<SOS>']]
    for word in text:
        indices.append(vocab[word])
    indices.append(vocab['<EOS>'])
    return indices

input_indices = [text_to_indices(seq, input_vocab) for seq in input_seqs]
target_indices = [text_to_indices(seq, output_vocab) for seq in target_seqs]

# 填充序列
def pad_sequences(sequences, max_length):
    padded_sequences = []
    for seq in sequences:
        if len(seq) < max_length:
            seq = seq + [input_vocab['<PAD>']] * (max_length - len(seq))
        padded_sequences.append(seq)
    return padded_sequences

max_input_length = max([len(seq) for seq in input_indices])
max_target_length = max([len(seq) for seq in target_indices])

input_padded = pad_sequences(input_indices, max_input_length)
target_padded = pad_sequences(target_indices, max_target_length)

# 转换为 PyTorch 张量
input_tensor = torch.tensor(input_padded, dtype=torch.long)
target_tensor = torch.tensor(target_padded, dtype=torch.long)

# 定义 LSTM 编码器
class EncoderLSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size)

    def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output, hidden = self.lstm(embedded, hidden)
        return output, hidden

    def initHidden(self):
        return (torch.zeros(1, 1, self.hidden_size),
                torch.zeros(1, 1, self.hidden_size))

# 定义 LSTM 解码器
class DecoderLSTM(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden):
        output = self.embedding(input).view(1, 1, -1)
        output = nn.functional.relu(output)
        output, hidden = self.lstm(output, hidden)
        output = self.softmax(self.out(output[0]))
        return output, hidden

    def initHidden(self):
        return (torch.zeros(1, 1, self.hidden_size),
                torch.zeros(1, 1, self.hidden_size))

# 训练参数
hidden_size = 256
encoder = EncoderLSTM(input_vocab_size, hidden_size)
decoder = DecoderLSTM(hidden_size, output_vocab_size)
criterion = nn.NLLLoss()
learning_rate = 0.01
encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)

# 训练循环
n_iters = 100
for iter in range(n_iters):
    for i in range(len(input_tensor)):
        encoder_optimizer.zero_grad()
        decoder_optimizer.zero_grad()

        input_seq = input_tensor[i]
        target_seq = target_tensor[i]

        encoder_hidden = encoder.initHidden()
        encoder_outputs = torch.zeros(max_input_length, encoder.hidden_size)

        for ei in range(len(input_seq)):
            encoder_output, encoder_hidden = encoder(input_seq[ei].unsqueeze(0), encoder_hidden)
            encoder_outputs[ei] = encoder_output[0, 0]

        decoder_input = torch.tensor([[output_vocab['<SOS>']]], dtype=torch.long)
        decoder_hidden = encoder_hidden

        loss = 0
        for di in range(len(target_seq)):
            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)
            loss += criterion(decoder_output, target_seq[di].unsqueeze(0))
            decoder_input = target_seq[di].unsqueeze(0)

        loss.backward()
        encoder_optimizer.step()
        decoder_optimizer.step()

    if iter % 10 == 0:
        print(f'Iteration {iter}, Loss: {loss.item()}')

    
```


## 附录

## PyTorch 的用法
### PyTorch 中的运算与模块
### 数据预处理
## 大语言模型
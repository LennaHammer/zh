---
layout: post
---


当初是为了自己背单词而使用的辅助工具，想了想竟然已经维护了有十年了。


主要功能是从文本中提取句子，用来在语境中背单词。但是被了一些单词书，感觉还是结合语境的容易记住。也能让挑选一些重要的单词出来重点学习。



目前位置写了三个版本
1. 用 bash 写的，因为 Stardford NLP 包提供了命令行接口，当时的 Python 包也是命令行上面包装了一层，而且当时机器性能不好，处理结果可以缓存，并不需要重复调用。
	1. 对文本分句，然后分词把词性变化转化为统一的形式。调用 Stardford NLP 得到的就是分词和 lemma 后的输出。
	2. 然后根据以上数据进行排版，输出结果有单词优先模式（给出单词然后例句，字母顺序），和例句优先模式（给出出现生词句子，然后把生词都标注出来加上中文解释）。
	3. 还可以生成 *MDict 2* 软件使用的词典数据包，在查单词时使用，或者利用该词典软件提供的阅读模式。
2. 第二个版本只有聚类模块，当时尝试了 1 rogot 2 threasure/wordnet 3 词向量，并参考词频（当时以 ANC 词频作为参考）。当时还是 rogot 版本的稍微好一点，对生僻词可以聚类到常见词下面，但是对常见词效果不好，因为同一个词会同时属于很多话题，而且 rogot 中话题之间也是相互关联的（有相互链接），导致常见词不断重复出现。rogot 太早缺少现代词汇问题，但是是用后两者方案进行补充，但是完全后两种方案的画，不像 rogot 有人工整理出来的目录可以当作聚类的中心。换用其他 threasure 差异有限。
4. 用 Python 重写，使用 Spacy 包（这个包出现的时间较晚），是目前维护的版本，重新实现了之前功能的一部分，并也尝试增强新功能，不过大多数修改还是在调整格式。
	1. 按照先词性再词频的排序
	2. 支持使用词典进行标注，目前使用的剑桥词典，因为有在线版。
	3. 拼写检查功能（根据词典，或者使用拼写检查库）
	4. 格式不知道怎么做好，调整了好几种想法，确实还需要调整。感觉生成 pdf 比较好，网页可能太大了浏览器会卡住。

其他尝试
- 生成背单词视频的功能。当时方案是把语音（来源于tts和词典）和图片（比如用的imagemagick）合并（ffmpeg）成视频。

目前处理的文本
1. 傲慢与偏见，小说。
2. 新概念英语，教材
3. 老友记第一季，，只保留对话
4. 走遍美国，教材
5. 爱丽丝，小说
6. 老托福听力文本，考试
7. 老英语 900 句，教材

每本的字数和单词量这里就不统计了。

通常认为出现次数大于等于2的单词会比较重要，可以过滤调低频的。

感觉这里可以列张表，再画一个图，看一些每个素材文本的单词量情况。然后对着一些总词频的数据看一下每本书用词的分布。还有口语和书面语用词习惯上有差异。

有一些文本可能还需要校对，也不排除有标点符号问题。